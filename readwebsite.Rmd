---
title: "Scraper"
author: "Dorothy Bishop"
date: '2023-01-01'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(stringr)
require(rvest)
```


```{r testread}

#modified from https://www.dataquest.io/blog/web-scraping-in-r-rvest/ plus stack_overflow!
#Demo
# simple <- read_html("https://dataquestio.github.io/web-scraping-pages/simple.html")
# 
# simple %>%
#   html_nodes("p") %>%
#   html_text()

#Now try Prime Scholars
h <- "https://www.primescholars.com/acta-psychopathology.html"
mytest <- read_html(h)
mytest %>%
  html_nodes("p") %>%
  html_text()

#Yes! This gets the list of editors with affiliations as entries 1-3, with \n divider before both name and affiliation
#Then the blurb, then current issue highlights

#Archive address for this journal is view-source:https://www.primescholars.com/acta-psychopathology/archive.html

a <- "https://www.primescholars.com/acta-psychopathology/archive.html"
myarch <- read_html(a)
myarch %>%
  html_nodes("p") %>%
  html_text()

#Site for each issue as https://www.primescholars.com/archive/ipap-volume-8-issue-10-year-2022.html
#need the equivalent code to 'ipap' for each journal. Can't deduce from the title, so this is now also in the excel sheet. 
i <- "https://www.primescholars.com/archive/ipap-volume-8-issue-10-year-2022.html"
myi <- read_html(i)
doilist<-myi %>%
  html_nodes("p") %>%
  html_text()
#This just gives dois

#but child page based on title (!) has name, affiliation and email!




```



We'll start with excel sheet and see if I can recreate the excel kind of structure for a journal not yet done.
Excel was created manually in initial explorations. 

This worked - had to add the short journal name to recognise correct html. Some do not have editors and those will have the instruction for submission recorded on df instead.

```{r dojournal}
index<-read.csv("PSindex.csv")  #from excel - dataframe with journal names
#colnames(index)[1:2]<-c('row','journal')

r1<-1 #start and end row
r2<-1


for (r in r1:r2){
myj <- index$journal[r]
# #Try just substituting hyphens for spaces in title
# myj2<-gsub(" ","-",myj)
#this didn't work for journal 11, so have added shortjournal column
#But kept this useful text!

myj2<-index$shortjournal[r]
edsite<-paste0("https://www.primescholars.com/",myj2,".html")


myed <- read_html(edsite)
myeds <- myed %>%
  html_nodes("p") %>%
  html_text()
#First 3 are editors
#Need to break at the \n point
for (e in 1:3){
  thised<-unlist(str_split(myeds[e],"\n"))[2]
  thisaf<-unlist(str_split(myeds[e],"\n"))[3]
  thiscol<-which(colnames(index)==paste0("Editor",e))
   thised<-gsub(",","",thised) #remove commas
   thisaf<-gsub(",",";",thisaf) #remove commas
  index[r,thiscol]<-thised
  index[r,(thiscol+1)]<-thisaf
}

}
write.csv(index,'PSindex.csv',row.names=F)


```

Now see if we can do other pages to list items in recent issues.
rs and those will have the instruction for submission recorded.

```{r doarticle}
#We'll write scraped information to a df based on the manual version from excel
index<-read.csv('PSindex.csv') #list with journals and editors and vol and issue nums


#done with affil 1 :11
#QUestion: crashes may have to do with affiliations? If author file not there
#4 crashed at issue 1
#7 crashed at issue 2 - has a lot of Oxford addresses
#8 crashed at vol 5 issue 3
#9 crashed at last vol
#10 crashed at 6, 1
#11 crashed at 6,3
#12 crashed at issue 3
#13 crashed at issue 3 (I did modify script to take 1st author only before this)
#15 crashed at issue 3
#16 crashed at issue 2
#17 crashed at issue 6
j1<-15
j2<-15 #range of journals to read

for (j in j1:j2){
  print(j)
  articledf<-read.csv("articles.csv")  #from excel - dataframe with col names, up to 100 rows

  myr<-index$shortjournal[j]
print(myr)
  archsite<-paste0("https://www.primescholars.com/",myr,"/archive.html")
  
  #Archsite doesn't help much.
  
  #Site for each issue as https://www.primescholars.com/archive/ipap-volume-8-issue-10-year-2022.html
  #the ip is common for all journals and then ap for initials of journal
  vol<-index$lastvol[j]
  r<-0 #initialise row counter
  for (issue in seq(index$lastissue[j],1,-1)){ #go backwards through issues
    
    myurl <- paste0("https://www.primescholars.com/archive/",index$initjournal[j],"-volume-",vol,"-issue-",issue,"-year-2022.html")
     Sys.sleep(1) #slow down to avoid getting throttled
     print(myurl)
    myi <- read_html(myurl)
    doilist<-myi %>%
      html_nodes("p") %>%
      html_text()
    #This just gives article type, titles and dois
    doiulist<-unlist(doilist)
    #Find those with DOI
    myDOI<-doiulist[grepl("DOI",doiulist)] #all DOIs for this issue
    
    #Get list of urls for individual articles!
    artlist<-myi %>%
      html_elements(".html")%>%
      html_attr("href")
    
  
    
    artulist<-unlist(artlist) #unlist
    
    #Now read url for each individual article
    n1<-1
    n2<-length(artulist)
    
    for (n in n1:n2){
      r<-r+1
      myu <- artulist[[n]]
      
       Sys.sleep(1) #slow down to avoid getting throttled
       print(myu)
      mya <- read_html(myu)
      mya2 <- mya %>%
        html_nodes("p") %>%
        html_text()
      
      #get the email for author
      myemail<- mya %>%
        html_nodes("i") %>%
        html_attr("title")
      mye<-which(!is.na(myemail))
      thisemail<-''
      if(length(mye)>0){thisemail<-myemail[mye]}
      
      #get the ID for author
      myid<- mya %>%
        html_nodes("a") %>%
        html_attr("id")
      
      w<-which(myid =='back-to-top')
      ww<-which(myid== 'navbarDropdownMenuLink')
       www<-which(myid== 'a1')
       
      myid<-myid[-c(w,ww,www)]
      myi<-which(!is.na(myid))
      thisid<-myid[myi]
      
      #get the address for author
          ### #Need the html code for this author to get the address
        authorlist<-mya %>%
      html_elements("a")%>%
      html_attr("href")
     #In this example, the author html address is row 28
     #But better to find it by string matching to avoid hard coding
     w<-which(grepl( "/author/", authorlist, fixed = TRUE)==TRUE) #seems clumsy but works
     authorurl<-authorlist[w]
     authorurl<-authorurl[1]
      Sys.sleep(1) #slow down to avoid getting throttled
      print(authorurl)
      tryCatch({
     myauthor<- read_html(authorurl)
    
     myau <- myauthor %>%
        html_nodes("p") %>%
        html_text()
     affil<-myau[1]},
      error=function(e){affil='x'})
     #remove /n
      thisaffil<-gsub("\n","",affil) #remove \n
      thisaffil<-gsub(";","",thisaffil) #remove ;
       thisaffil<-gsub(",","_",thisaffil) #change , to _
        ###
      
      
      #get the title
      myt<- mya %>%
        html_nodes("h5") %>%
        html_text()
      
      print(myt)
      print(thisaffil)
      #Write the scraped information to data frame
      articledf$vol[r]<-index$lastvol[j]
      articledf$issue[r]<-issue
      articledf$title[r]<-myt
      articledf$Nauthors[r]<-length(thisid)
      articledf$author[r]<-thisid[1]
      articledf$email.in.pdf[r]<-thisemail
      articledf$doi[r]<-myDOI[n]
      articledf$Journal[r]<-index$journal[j]
      articledf$affiliation[r]<-thisaffil
      
    }
  }
  
  myfilename<-paste0('PSarticles_',index$shortjournal[j],'.csv')
  
  write.csv(articledf,myfilename,row.names=F)
}

```
